{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6708b1a7",
   "metadata": {},
   "source": [
    "### Unique word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a3942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique gloss words: 960\n",
      "Unique sentence words: 4196\n",
      "Total gloss words: 9177\n",
      "Total sentence words: 8926\n",
      "\n",
      "Line count comparison per ID:\n",
      "    gloss_lines  sentence_lines  match\n",
      "id                                    \n",
      "1           359             359   True\n",
      "2           446             446   True\n",
      "3           159             159   True\n",
      "4           439             439   True\n",
      "5           191             191   True\n",
      "6           319             319   True\n",
      "7             8               8   True\n",
      "8             9               9   True\n",
      "9            70              70   True\n",
      "\n",
      "✅ Total lines: 2000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Helped function\n",
    "def read_files(pattern):\n",
    "    rows = []\n",
    "    for path in glob.glob(pattern):\n",
    "        file_id = os.path.splitext(os.path.basename(path))[0].split(\"_\")[-1]\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                rows.append({\n",
    "                    \"id\": file_id,\n",
    "                    \"line\": line.strip()\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def explode_words(df, text_col=\"line\"):\n",
    "    return (\n",
    "        df.assign(word=df[text_col].str.split())\n",
    "          .explode(\"word\")\n",
    "          .dropna(subset=[\"word\"])\n",
    "    )\n",
    "\n",
    "# Count and print word counts\n",
    "gloss_df = read_files(\"../txt/lsl_glosses_*.txt\")\n",
    "sent_df  = read_files(\"../txt/latvian_sentences_*.txt\")\n",
    "\n",
    "gloss_unique_words = explode_words(gloss_df)[\"word\"].nunique()\n",
    "sent_unique_words  = explode_words(sent_df)[\"word\"].nunique()\n",
    "\n",
    "gloss_total_words = explode_words(gloss_df).shape[0]\n",
    "sent_total_words  = explode_words(sent_df).shape[0]\n",
    "\n",
    "print(f\"Unique gloss words: {gloss_unique_words}\")\n",
    "print(f\"Unique sentence words: {sent_unique_words}\")\n",
    "print(f\"Total gloss words: {gloss_total_words}\")\n",
    "print(f\"Total sentence words: {sent_total_words}\")\n",
    "\n",
    "# Compare line count\n",
    "gloss_lines = gloss_df.groupby(\"id\").size().rename(\"gloss_lines\")\n",
    "sent_lines  = sent_df.groupby(\"id\").size().rename(\"sentence_lines\")\n",
    "\n",
    "line_check = (\n",
    "    pd.concat([gloss_lines, sent_lines], axis=1)\n",
    "      .fillna(0)\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "line_check[\"match\"] = line_check[\"gloss_lines\"] == line_check[\"sentence_lines\"]\n",
    "\n",
    "print(\"\\nLine count comparison per ID:\")\n",
    "print(line_check)\n",
    "\n",
    "# Print total line count\n",
    "total_gloss_lines = len(gloss_df)\n",
    "total_sentence_lines = len(sent_df)\n",
    "\n",
    "if total_gloss_lines == total_sentence_lines:\n",
    "    print(f\"\\n✅ Total lines: {total_gloss_lines}\")\n",
    "else:\n",
    "    print(f\"\\n❌ Sentence lines: {total_sentence_lines}, gloss lines: {total_gloss_lines}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b2eb2",
   "metadata": {},
   "source": [
    "### Check sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af15e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mismatches found: 1\n",
      "------------------------------------------------------------\n",
      "File: latvian_sentences_4.txt | Line: 143\n",
      "Sentence (1): Pavasarī mums ir prieks dāvāt ziedus un skaistus vārdus mūsu tuvajiem cilvēkiem.\n",
      "Gloss    (2): pavasaris mums ir prieks dāvināt zieds un skaists vārds mums tuvs cilvēki\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "LV_PATTERN = \"../txt/latvian_sentences_*.txt\"\n",
    "GLOSS_PATTERN = \"../txt/lsl_glosses_*.txt\"\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Lowercase and split into word tokens.\n",
    "    Keeps letters, digits, and brackets like [NAME].\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\b[\\w\\[\\]]+\\b\", text.lower())\n",
    "\n",
    "lv_files = sorted(glob.glob(LV_PATTERN))\n",
    "gloss_files = sorted(glob.glob(GLOSS_PATTERN))\n",
    "\n",
    "assert len(lv_files) == len(gloss_files), \"Mismatch in number of sentence and gloss files\"\n",
    "\n",
    "mismatches = []\n",
    "\n",
    "for lv_path, gloss_path in zip(lv_files, gloss_files):\n",
    "    with open(lv_path, \"r\", encoding=\"utf-8\") as lv_f, \\\n",
    "         open(gloss_path, \"r\", encoding=\"utf-8\") as gloss_f:\n",
    "\n",
    "        lv_lines = lv_f.readlines()\n",
    "        gloss_lines = gloss_f.readlines()\n",
    "\n",
    "    assert len(lv_lines) == len(gloss_lines), (\n",
    "        f\"Line count mismatch in {Path(lv_path).name} and {Path(gloss_path).name}\"\n",
    "    )\n",
    "\n",
    "    for idx, (lv_line, gloss_line) in enumerate(zip(lv_lines, gloss_lines), start=1):\n",
    "        lv_tokens = tokenize(lv_line)\n",
    "        gloss_tokens = tokenize(gloss_line)\n",
    "\n",
    "        lv_ir_count = lv_tokens.count(\"mums\")\n",
    "        gloss_ir_count = gloss_tokens.count(\"mums\")\n",
    "\n",
    "        if lv_ir_count != gloss_ir_count:\n",
    "            mismatches.append({\n",
    "                \"file\": Path(lv_path).name,\n",
    "                \"line_number\": idx,\n",
    "                \"sentence\": lv_line.strip(),\n",
    "                \"gloss\": gloss_line.strip(),\n",
    "                \"sentence_ir_count\": lv_ir_count,\n",
    "                \"gloss_ir_count\": gloss_ir_count,\n",
    "            })\n",
    "\n",
    "print(f\"Total mismatches found: {len(mismatches)}\")\n",
    "\n",
    "for m in mismatches[:10]:\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"File: {m['file']} | Line: {m['line_number']}\")\n",
    "    print(f\"Sentence ({m['sentence_ir_count']}): {m['sentence']}\")\n",
    "    print(f\"Gloss    ({m['gloss_ir_count']}): {m['gloss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83502878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mismatches found: 1\n",
      "------------------------------------------------------------\n",
      "File: latvian_sentences_1.txt | Line: 88\n",
      "Sentence (1): Es jūtos labi.\n",
      "Gloss    (0): es justies slikts\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "LV_PATTERN = \"../txt/latvian_sentences_*.txt\"\n",
    "GLOSS_PATTERN = \"../txt/lsl_glosses_*.txt\"\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b[\\w\\[\\]]+\\b\", text.lower())\n",
    "\n",
    "lv_files = sorted(glob.glob(LV_PATTERN))\n",
    "gloss_files = sorted(glob.glob(GLOSS_PATTERN))\n",
    "\n",
    "assert len(lv_files) == len(gloss_files), \"Mismatch in number of sentence and gloss files\"\n",
    "\n",
    "# TARGET_FORMS = {\"esmu\", \"esam\", \"esi\"}\n",
    "# GLOSS_FORM = \"esmu\"\n",
    "\n",
    "# TARGET_FORMS = {\"labs\", \"labi\", \"laba\"}\n",
    "# GLOSS_FORM = \"labs\"\n",
    "\n",
    "# TARGET_FORMS = {\"tas\", \"tur\", \"turieni\", \"tā\", \"šī\", \"šis\", \"to\"}\n",
    "# GLOSS_FORM = \"tas\"\n",
    "\n",
    "TARGET_FORMS = {\"labs\", \"labi\", \"laba\"}\n",
    "GLOSS_FORM = \"labs\"\n",
    "\n",
    "mismatches = []\n",
    "\n",
    "for lv_path, gloss_path in zip(lv_files, gloss_files):\n",
    "    with open(lv_path, \"r\", encoding=\"utf-8\") as lv_f, \\\n",
    "         open(gloss_path, \"r\", encoding=\"utf-8\") as gloss_f:\n",
    "\n",
    "        lv_lines = lv_f.readlines()\n",
    "        gloss_lines = gloss_f.readlines()\n",
    "\n",
    "    assert len(lv_lines) == len(gloss_lines), (\n",
    "        f\"Line count mismatch in {Path(lv_path).name} and {Path(gloss_path).name}\"\n",
    "    )\n",
    "\n",
    "    for idx, (lv_line, gloss_line) in enumerate(zip(lv_lines, gloss_lines), start=1):\n",
    "        lv_tokens = tokenize(lv_line)\n",
    "        gloss_tokens = tokenize(gloss_line)\n",
    "\n",
    "        sentence_count = sum(lv_tokens.count(form) for form in TARGET_FORMS)\n",
    "        gloss_count = gloss_tokens.count(GLOSS_FORM)\n",
    "\n",
    "        if sentence_count > 0 and sentence_count != gloss_count:\n",
    "            mismatches.append({\n",
    "                \"file\": Path(lv_path).name,\n",
    "                \"line_number\": idx,\n",
    "                \"sentence\": lv_line.strip(),\n",
    "                \"gloss\": gloss_line.strip(),\n",
    "                \"sentence_count\": sentence_count,\n",
    "                \"gloss_count\": gloss_count,\n",
    "            })\n",
    "\n",
    "print(f\"Total mismatches found: {len(mismatches)}\")\n",
    "\n",
    "for m in mismatches[:10]:\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"File: {m['file']} | Line: {m['line_number']}\")\n",
    "    print(f\"Sentence ({m['sentence_count']}): {m['sentence']}\")\n",
    "    print(f\"Gloss    ({m['gloss_count']}): {m['gloss']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e36be3",
   "metadata": {},
   "source": [
    "### Counts gloss instances of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e4d5501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    aizpildīt\n",
      "      aizstāt\n",
      "           ak\n",
      "       apkārt\n",
      "     apraksts\n",
      "     apvienot\n",
      "       armija\n",
      "     atbalsts\n",
      "  atgriezties\n",
      "        atkal\n",
      "       atpūta\n",
      "    attīstība\n",
      "      atļauja\n",
      "    atšķirība\n",
      "      avokado\n",
      "        avīze\n",
      "        burti\n",
      "         cept\n",
      "       cukurs\n",
      "      cēlonis\n",
      "       dalība\n",
      "       dators\n",
      "     deficīts\n",
      "        dejot\n",
      "     depozīts\n",
      "        dievs\n",
      "       drīkst\n",
      "        dzeja\n",
      "      dziedāt\n",
      "      dziesma\n",
      "       dāvana\n",
      "    ekonomika\n",
      "   emocionāls\n",
      "   evakuācija\n",
      "         eļļa\n",
      "      fizisks\n",
      "     francija\n",
      "      godināt\n",
      "   greipfrūts\n",
      "      gruzija\n",
      "      grāmata\n",
      "        ieeja\n",
      "       indīgs\n",
      "    infekcija\n",
      "      ingvers\n",
      "  instrukcija\n",
      "  interesants\n",
      "    internets\n",
      "      izdomāt\n",
      "      izmaksa\n",
      "      izmisis\n",
      "  izvairīties\n",
      "       jebkur\n",
      "         joks\n",
      "jēzus_kristus\n",
      "       kafija\n",
      "        kalns\n",
      "        kauls\n",
      "         kaut\n",
      "        klase\n",
      "       kleita\n",
      "      klimats\n",
      "     klātiene\n",
      " komunikācija\n",
      "     koncerts\n",
      "    konflikts\n",
      "     konkrēts\n",
      "     kontrole\n",
      "       kopējs\n",
      "       krasts\n",
      "     krievija\n",
      "       krūtis\n",
      "         kūka\n",
      "        laime\n",
      "        lasīt\n",
      "     latvijas\n",
      "         lemt\n",
      "      lietuva\n",
      "         likt\n",
      "         lokā\n",
      "        maize\n",
      "         mala\n",
      "      mazināt\n",
      "      mellene\n",
      "     miegains\n",
      "        miegs\n",
      "       mūzika\n",
      "     nebraukt\n",
      "       neilgs\n",
      "         nekā\n",
      "        nervi\n",
      "   nevajadzēt\n",
      "      neviens\n",
      "     noguldīt\n",
      "    norvēģija\n",
      "        olīva\n",
      "    operācija\n",
      " organizācija\n",
      "       palikt\n",
      "     palīdzēt\n",
      "     paraksts\n",
      "      parasti\n",
      "         pase\n",
      "     pasniegt\n",
      "      pavisam\n",
      "   personisks\n",
      "   pieklājība\n",
      "    pieprasīt\n",
      "    pieraksts\n",
      "   pieslēgums\n",
      "      pietikt\n",
      "     piešķirt\n",
      "      piliens\n",
      "      plaušas\n",
      "         plus\n",
      "       plānot\n",
      "    portugāle\n",
      "     procents\n",
      " proklamācija\n",
      "        prāts\n",
      "     publicēt\n",
      "     pārējais\n",
      "         pēda\n",
      "     ražošana\n",
      "      risināt\n",
      "       rīcība\n",
      "  saimniecība\n",
      "     sirsnīgs\n",
      "        skaņa\n",
      "       slapjš\n",
      "      slidens\n",
      "        slogs\n",
      "    slovēnija\n",
      "        smags\n",
      "   specifisks\n",
      "       spēlēt\n",
      "    steigties\n",
      "       stress\n",
      "  svārstīties\n",
      "   teritorija\n",
      "           to\n",
      "      treniņš\n",
      "     turpināt\n",
      "     tīrīšana\n",
      "        urīns\n",
      "   uz_priekšu\n",
      "      valdība\n",
      "        vidus\n",
      "      vienmēr\n",
      "   vingrošana\n",
      "         virs\n",
      "      vēstule\n",
      "         zars\n",
      "       zudums\n",
      "    zviedrija\n",
      "         ērts\n",
      "      īpašums\n",
      "\n",
      "Total words printed: 157\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Read all gloss files\n",
    "words = []\n",
    "for path in glob(\"../txt/lsl_glosses_*.txt\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        words.extend(f.read().split())\n",
    "\n",
    "df = pd.DataFrame(words, columns=[\"word\"])\n",
    "\n",
    "word_counts = df[\"word\"].value_counts().reset_index()\n",
    "word_counts.columns = [\"word\", \"count\"]\n",
    "\n",
    "filtered = word_counts[word_counts[\"count\"] <= 1]\n",
    "filtered = filtered.sort_values(by=\"word\")\n",
    "\n",
    "print(filtered[\"word\"].to_string(index=False))\n",
    "print(\"\\nTotal words printed:\", len(filtered))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
