{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ead43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded 2 batches with 272 total pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "The tokenizer you are loading from '../mt5-pruned' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model loaded on: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. File patterns\n",
    "LV_PATTERN = \"../txt/latvian_sentences_*.txt\"\n",
    "GLOSS_PATTERN = \"../txt/lsl_glosses_*.txt\"\n",
    "PRUNED_MODEL_PATH = \"../mt5-pruned\"\n",
    "\n",
    "# 2. Append all files to master list\n",
    "lv_lines = []\n",
    "gloss_lines = []\n",
    "total_files = 0\n",
    "\n",
    "for file_path in sorted(glob.glob(LV_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lv_lines.extend([line.strip() for line in f if line.strip()])\n",
    "    total_files += 1\n",
    "\n",
    "for file_path in sorted(glob.glob(GLOSS_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gloss_lines.extend([line.strip() for line in f if line.strip()])\n",
    "\n",
    "# 3. Check validity\n",
    "assert len(lv_lines) == len(gloss_lines), f\"❌ Mismatch! LV lines: {len(lv_lines)}, Gloss lines: {len(gloss_lines)}\"\n",
    "print(f\"✅ Successfully loaded {total_files} batches with {len(lv_lines)} total pairs.\")\n",
    "\n",
    "# 4. Create a dataset\n",
    "data = {\"lv\": lv_lines, \"gloss\": gloss_lines}\n",
    "raw_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Split into Train (90%) and Test (10%) so we can verify learning\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.1)\n",
    "# print(\"Data split:\", split_dataset)\n",
    "\n",
    "# 5. Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_PATH)\n",
    "config = AutoConfig.from_pretrained(PRUNED_MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "state_dict = torch.load(os.path.join(PRUNED_MODEL_PATH, \"pytorch_model.bin\"))\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Pruned Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/244 [00:00<?, ? examples/s]c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 244/244 [00:00<00:00, 13006.75 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 4584.83 examples/s]\n",
      "C:\\Users\\liene\\AppData\\Local\\Temp\\ipykernel_12888\\2491706255.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "# Re-define preprocess to be sure\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"lv\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"gloss\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# --- 2. AGGRESSIVE TRAINING ARGUMENTS ---\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-lsl-model\",\n",
    "    eval_strategy=\"no\",             # Skip eval to speed it up\n",
    "    save_strategy=\"no\",             # Don't save checkpoints yet\n",
    "    learning_rate=1e-3,             # MUCH HIGHER (was 2e-5)\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=50,            # LONGER (was 20)\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adafactor\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    # We train on the WHOLE dataset to force memorization for this test\n",
    "    # (We are temporarily ignoring the test split to ensure it learns)\n",
    "    eval_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b1928",
   "metadata": {},
   "source": [
    "# -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d768999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab map. Remapping 608 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 244/244 [00:00<00:00, 18051.15 examples/s]\n",
      "Map: 100%|██████████| 28/28 [00:00<00:00, 6243.51 examples/s]\n",
      "C:\\Users\\liene\\AppData\\Local\\Temp\\ipykernel_12888\\3997311478.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# --- 1. LOAD THE MAP (The \"Rosetta Stone\") ---\n",
    "# This file tells us: \"Old ID 15020 is now New ID 5\"\n",
    "with open(os.path.join(PRUNED_MODEL_PATH, \"vocab_map.json\"), \"r\") as f:\n",
    "    # The JSON is saved as { \"new_id\": old_id }, so we reverse it.\n",
    "    new2old_map = json.load(f)\n",
    "    old2new_map = {v: int(k) for k, v in new2old_map.items()}\n",
    "\n",
    "# Find the \"New\" ID for the Unknown token (UNK)\n",
    "# We need this for words like \"translate\" if they weren't in your pruning text\n",
    "original_unk_id = tokenizer.unk_token_id\n",
    "new_unk_id = old2new_map.get(original_unk_id, 0) # Default to 0 if weirdness happens\n",
    "\n",
    "print(f\"Loaded vocab map. Remapping {len(old2new_map)} tokens.\")\n",
    "\n",
    "# --- 2. DEFINE THE REMAPPING FUNCTION ---\n",
    "def remap_tokens(token_ids):\n",
    "    # Convert list of Old IDs to New IDs\n",
    "    # If a token wasn't in our pruning list, turn it into UNK (new_unk_id)\n",
    "    return [old2new_map.get(tid, new_unk_id) for tid in token_ids]\n",
    "\n",
    "# --- 3. UPDATED PREPROCESS FUNCTION ---\n",
    "def preprocess_function(examples):\n",
    "    # A. Tokenize Inputs (Standard way - produces HUGE IDs)\n",
    "    inputs = [\"translate Latvian to Gloss: \" + ex for ex in examples[\"lv\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    # B. Tokenize Targets (Standard way - produces HUGE IDs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"gloss\"], max_length=128, truncation=True)\n",
    "\n",
    "    # C. MANUAL REMAPPING STEP (The Fix!)\n",
    "    # We replace the huge IDs with the tiny mapped IDs\n",
    "    model_inputs[\"input_ids\"] = [remap_tokens(ids) for ids in model_inputs[\"input_ids\"]]\n",
    "    \n",
    "    # We must also remap the labels!\n",
    "    model_inputs[\"labels\"] = [remap_tokens(ids) for ids in labels[\"input_ids\"]]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the new function\n",
    "tokenized_datasets = split_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# --- 4. TRAINING ARGUMENTS ---\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-lsl-model\",\n",
    "    eval_strategy=\"no\",             \n",
    "    save_strategy=\"no\",             \n",
    "    learning_rate=1e-3,             \n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=50,            \n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adafactor\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TRAIN ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb40578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model...\")\n",
    "\n",
    "save_dir = \"E:/Documents/GitHub/LSL/mt5-lsl-model\"\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62dc446",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be75ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "\n",
      "Input:  Mans vārds ir [NAME].\n",
      "Result: mans vārds ir [NAME]\n",
      "\n",
      "Input:  Vai tu esi labs cilvēks?\n",
      "Result: vai tu esmu labs\n",
      "\n",
      "Input:  Man ir ļoti lielas mājas.\n",
      "Result: mans ir ļoti liels māja\n",
      "\n",
      "Input:  Sveika!\n",
      "Result: sveiks\n",
      "\n",
      "Input:  Labdien, [NAME]!\n",
      "Result: labdien [NAME]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP MAPS (Crucial!) ---\n",
    "# We need to load the map to convert between \"Big Tokenizer\" and \"Small Model\"\n",
    "vocab_map_path = os.path.join(PRUNED_MODEL_PATH, \"vocab_map.json\")\n",
    "\n",
    "with open(vocab_map_path, \"r\") as f:\n",
    "    new2old_map = json.load(f)\n",
    "    # We need both directions!\n",
    "    old2new_map = {int(v): int(k) for k, v in new2old_map.items()} # Big -> Small\n",
    "    new2old_map = {int(k): int(v) for k, v in new2old_map.items()} # Small -> Big\n",
    "\n",
    "# Identify the UNK token ID in the new mapping\n",
    "# If a word (like \"translate\") isn't in our map, we point it to the pruned UNK ID.\n",
    "# Usually, UNK is ID 2 in standard T5, let's find where ID 2 went.\n",
    "original_unk_id = tokenizer.unk_token_id\n",
    "pruned_unk_id = old2new_map.get(original_unk_id, 0) # Fallback to 0 if not found\n",
    "\n",
    "# --- 2. CUSTOM TRANSLATION FUNCTION ---\n",
    "def predict_gloss(text):\n",
    "    # A. Prepare Input: Simply use the raw text\n",
    "    full_text = text\n",
    "    \n",
    "    # B. Tokenize (Get Big IDs)\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    input_ids = inputs.input_ids[0].tolist()\n",
    "\n",
    "    # C. Remap Input (Big IDs -> Small IDs)\n",
    "    pruned_input_ids = [old2new_map.get(tid, pruned_unk_id) for tid in input_ids]\n",
    "    \n",
    "    # Convert back to tensor and move to GPU\n",
    "    input_tensor = torch.tensor([pruned_input_ids]).to(model.device)\n",
    "\n",
    "    # D. Generate (Model produces Small IDs)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_tensor, max_new_tokens=128)\n",
    "    \n",
    "    # E. Remap Output (Small IDs -> Big IDs)\n",
    "    output_ids = outputs[0].tolist()\n",
    "    original_output_ids = [new2old_map.get(tid, tokenizer.unk_token_id) for tid in output_ids]\n",
    "\n",
    "    # F. Decode\n",
    "    result = tokenizer.decode(original_output_ids, skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "# --- 3. RUN TESTS ---\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "\n",
    "test_sentences = [\n",
    "    lv_lines[10],\n",
    "    \"Vai tu esi labs cilvēks?\",\n",
    "    \"Man ir ļoti lielas mājas.\",\n",
    "    \"Sveika!\",\n",
    "    \"Labdien, [NAME]!\"\n",
    "]\n",
    "\n",
    "for text in test_sentences:\n",
    "    gloss = predict_gloss(text)\n",
    "    print(f\"\\nInput:  {text}\")\n",
    "    print(f\"Result: {gloss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
