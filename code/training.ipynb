{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ead43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded 3 batches with 630 total pairs.\n",
      "Data split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['lv', 'gloss'],\n",
      "        num_rows: 567\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['lv', 'gloss'],\n",
      "        num_rows: 63\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "The tokenizer you are loading from '../mt5-pruned' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model loaded on: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. File patterns\n",
    "LV_PATTERN = \"../txt/latvian_sentences_*.txt\"\n",
    "GLOSS_PATTERN = \"../txt/lsl_glosses_*.txt\"\n",
    "PRUNED_MODEL_PATH = \"../mt5-pruned\"\n",
    "\n",
    "# 2. Append all files to master list\n",
    "lv_lines = []\n",
    "gloss_lines = []\n",
    "total_files = 0\n",
    "\n",
    "for file_path in sorted(glob.glob(LV_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lv_lines.extend([line.strip() for line in f if line.strip()])\n",
    "    total_files += 1\n",
    "\n",
    "for file_path in sorted(glob.glob(GLOSS_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gloss_lines.extend([line.strip() for line in f if line.strip()])\n",
    "\n",
    "# 3. Check validity\n",
    "assert len(lv_lines) == len(gloss_lines), f\"❌ Mismatch! Sentence lines: {len(lv_lines)}, gloss lines: {len(gloss_lines)}\"\n",
    "print(f\"✅ Successfully loaded {total_files} batches with {len(lv_lines)} total pairs.\")\n",
    "\n",
    "# 4. Create a dataset\n",
    "data = {\"lv\": lv_lines, \"gloss\": gloss_lines}\n",
    "raw_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Split into Train (90%) and Test (10%) so we can verify learning\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.1)\n",
    "print(\"Data split:\", split_dataset)\n",
    "\n",
    "# 5. Load model\n",
    "tokenizer = T5Tokenizer.from_pretrained(PRUNED_MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(PRUNED_MODEL_PATH)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Pruned Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d768999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab map. Remapping 1210 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 567/567 [00:00<00:00, 23602.56 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 10492.42 examples/s]\n",
      "C:\\Users\\liene\\AppData\\Local\\Temp\\ipykernel_9404\\3046864190.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# --- 1. LOAD THE MAP (The \"Rosetta Stone\") ---\n",
    "# This file tells us: \"Old ID 15020 is now New ID 5\"\n",
    "with open(os.path.join(PRUNED_MODEL_PATH, \"vocab_map.json\"), \"r\") as f:\n",
    "    # The JSON is saved as { \"new_id\": old_id }, so we reverse it.\n",
    "    new2old_map = json.load(f)\n",
    "    old2new_map = {v: int(k) for k, v in new2old_map.items()}\n",
    "\n",
    "# Find the \"New\" ID for the Unknown token (UNK)\n",
    "# We need this for words like \"translate\" if they weren't in your pruning text\n",
    "original_unk_id = tokenizer.unk_token_id\n",
    "new_unk_id = old2new_map.get(original_unk_id, 0) # Default to 0 if weirdness happens\n",
    "\n",
    "print(f\"Loaded vocab map. Remapping {len(old2new_map)} tokens.\")\n",
    "\n",
    "# --- 2. DEFINE THE REMAPPING FUNCTION ---\n",
    "def remap_tokens(token_ids):\n",
    "    # Convert list of Old IDs to New IDs\n",
    "    # If a token wasn't in our pruning list, turn it into UNK (new_unk_id)\n",
    "    return [old2new_map.get(tid, new_unk_id) for tid in token_ids]\n",
    "\n",
    "# --- 3. UPDATED PREPROCESS FUNCTION ---\n",
    "def preprocess_function(examples):\n",
    "    # A. Tokenize Inputs (Standard way - produces HUGE IDs)\n",
    "    # inputs = [\"translate Latvian to Gloss: \" + ex for ex in examples[\"lv\"]]\n",
    "    inputs = examples[\"lv\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    # B. Tokenize Targets (Standard way - produces HUGE IDs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"gloss\"], max_length=128, truncation=True)\n",
    "\n",
    "    # C. MANUAL REMAPPING STEP (The Fix!)\n",
    "    # We replace the huge IDs with the tiny mapped IDs\n",
    "    model_inputs[\"input_ids\"] = [remap_tokens(ids) for ids in model_inputs[\"input_ids\"]]\n",
    "    \n",
    "    # We must also remap the labels!\n",
    "    model_inputs[\"labels\"] = [remap_tokens(ids) for ids in labels[\"input_ids\"]]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the new function\n",
    "tokenized_datasets = split_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# --- 4. TRAINING ARGUMENTS ---\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../mt5-lsl-model\",\n",
    "    learning_rate=1e-3,        # Used to be 1e-3\n",
    "    num_train_epochs=50,       # can be high, but early stopping will prevent overfitting\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,             # evaluate frequently, since dataset is tiny\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000000,\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # track validation loss\n",
    "    greater_is_better=False,\n",
    "    predict_with_generate=True,\n",
    "    optim=\"adafactor\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a06fc95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[510, 409, 495, 865, 260, 257, 1]\n",
      "[575, 663, 495, 865, 260, 1]\n",
      "Max input id: 865\n",
      "Max label id: 865\n",
      "Vocab size: 1210\n",
      "Dataset vocab size: 955\n",
      "Pruned vocab size: 1210\n"
     ]
    }
   ],
   "source": [
    "sample = tokenized_datasets[\"train\"][1]\n",
    "print(sample[\"input_ids\"][:20])\n",
    "print(sample[\"labels\"][:20])\n",
    "print(\"Max input id:\", max(sample[\"input_ids\"]))\n",
    "print(\"Max label id:\", max(sample[\"labels\"]))\n",
    "print(\"Vocab size:\", model.config.vocab_size)\n",
    "\n",
    "token_set = set()\n",
    "for line in lv_lines + gloss_lines:\n",
    "    token_set.update(tokenizer(line)['input_ids'])\n",
    "print(\"Dataset vocab size:\", len(token_set))\n",
    "print(\"Pruned vocab size:\", len(old2new_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f3f3f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='3550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  95/3550 00:55 < 34:09, 1.69 it/s, Epoch 1/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.431396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>14.615900</td>\n",
       "      <td>7.898682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>14.615900</td>\n",
       "      <td>6.731386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.392400</td>\n",
       "      <td>6.419647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>9.392400</td>\n",
       "      <td>6.690964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.305200</td>\n",
       "      <td>4.966996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>8.305200</td>\n",
       "      <td>4.557003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.488600</td>\n",
       "      <td>6.201177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>7.488600</td>\n",
       "      <td>4.727540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.849400</td>\n",
       "      <td>4.498833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>6.849400</td>\n",
       "      <td>4.347504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.789700</td>\n",
       "      <td>4.153176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>5.789700</td>\n",
       "      <td>3.995070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>5.251900</td>\n",
       "      <td>3.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>5.251900</td>\n",
       "      <td>4.531258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.013500</td>\n",
       "      <td>4.415956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>5.013500</td>\n",
       "      <td>4.353283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.714400</td>\n",
       "      <td>4.148087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>5.714400</td>\n",
       "      <td>3.985275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=95, training_loss=7.467303627415707, metrics={'train_runtime': 55.8699, 'train_samples_per_second': 507.428, 'train_steps_per_second': 63.54, 'total_flos': 2937432502272.0, 'train_loss': 7.467303627415707, 'epoch': 1.3380281690140845})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. TRAIN ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb40578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model...\")\n",
    "\n",
    "save_dir = \"E:/Documents/GitHub/LSL/mt5-lsl-model\"\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62dc446",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be75ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded 3 batches with 630 total pairs.\n",
      "\n",
      "--- RESULTS ---\n",
      "\n",
      "Input:  Čau!\n",
      "Result: vai Čas\n",
      "\n",
      "Input:  Kāds ir jūsu vārds?\n",
      "Result: vai jūs ir jūsu vārds\n",
      "\n",
      "Input:  Kāds ir tavs vārds?\n",
      "Result: mans ir tavs vārds\n",
      "\n",
      "Input:  Priecājos ar Jums iepazīties!\n",
      "Result: mans iepazīties\n",
      "\n",
      "Input:  Man iet labi.\n",
      "Result: man iet labi\n",
      "\n",
      "Input:  Vai tu esi labs cilvēks?\n",
      "Result: vai tu esi labs cilvēks\n",
      "\n",
      "Input:  Man ir ļoti lielas mājas.\n",
      "Result: mans ir liels māja\n",
      "\n",
      "Input:  Sveika!\n",
      "Result: vai Sveika\n",
      "\n",
      "Input:  Labdien, [NAME]!\n",
      "Result: [NAME]\n",
      "\n",
      "Input:  [NAME] ir īss...\n",
      "Result: [NAME] ir īss\n",
      "\n",
      "Input:  Braucam pie zaļās gaismas.\n",
      "Result: vai zaļās gaismas\n",
      "\n",
      "Input:  Viņš nopirka divus kreklus\n",
      "Result: vai nopirkt krekls krevs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- 1. SETUP MAPS (Crucial!) ---\n",
    "# We need to load the map to convert between \"Big Tokenizer\" and \"Small Model\"\n",
    "vocab_map_path = os.path.join(PRUNED_MODEL_PATH, \"vocab_map.json\")\n",
    "\n",
    "with open(vocab_map_path, \"r\") as f:\n",
    "    new2old_map = json.load(f)\n",
    "    # We need both directions!\n",
    "    old2new_map = {int(v): int(k) for k, v in new2old_map.items()} # Big -> Small\n",
    "    new2old_map = {int(k): int(v) for k, v in new2old_map.items()} # Small -> Big\n",
    "\n",
    "# Identify the UNK token ID in the new mapping\n",
    "# If a word (like \"translate\") isn't in our map, we point it to the pruned UNK ID.\n",
    "# Usually, UNK is ID 2 in standard T5, let's find where ID 2 went.\n",
    "original_unk_id = tokenizer.unk_token_id\n",
    "pruned_unk_id = old2new_map.get(original_unk_id, 0) # Fallback to 0 if not found\n",
    "\n",
    "\n",
    "# 1. File patterns\n",
    "LV_PATTERN = \"../txt/latvian_sentences_*.txt\"\n",
    "GLOSS_PATTERN = \"../txt/lsl_glosses_*.txt\"\n",
    "PRUNED_MODEL_PATH = \"../mt5-pruned\"\n",
    "\n",
    "# 2. Append all files to master list\n",
    "lv_lines = []\n",
    "gloss_lines = []\n",
    "total_files = 0\n",
    "\n",
    "for file_path in sorted(glob.glob(LV_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lv_lines.extend([line.strip() for line in f if line.strip()])\n",
    "    total_files += 1\n",
    "\n",
    "for file_path in sorted(glob.glob(GLOSS_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gloss_lines.extend([line.strip() for line in f if line.strip()])\n",
    "\n",
    "# 3. Check validity\n",
    "assert len(lv_lines) == len(gloss_lines), f\"❌ Mismatch! Sentence lines: {len(lv_lines)}, gloss lines: {len(gloss_lines)}\"\n",
    "print(f\"✅ Successfully loaded {total_files} batches with {len(lv_lines)} total pairs.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. CUSTOM TRANSLATION FUNCTION ---\n",
    "def predict_gloss(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    input_ids = inputs.input_ids[0].tolist()\n",
    "    \n",
    "    # Remap to pruned IDs\n",
    "    pruned_input_ids = [old2new_map.get(tid, pruned_unk_id) for tid in input_ids]\n",
    "    input_tensor = torch.tensor([pruned_input_ids]).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_tensor,\n",
    "            max_length=32,\n",
    "            num_beams=3,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # Remap output IDs back to full tokenizer\n",
    "    output_ids = outputs[0].tolist()\n",
    "    original_output_ids = [new2old_map.get(tid, tokenizer.unk_token_id) for tid in output_ids]\n",
    "\n",
    "    # Decode\n",
    "    return tokenizer.decode(original_output_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# --- 3. RUN TESTS ---\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "\n",
    "test_sentences = [\n",
    "    lv_lines[10],\n",
    "    lv_lines[20],\n",
    "    lv_lines[30],\n",
    "    lv_lines[40],\n",
    "    lv_lines[50]\n",
    "]\n",
    "\n",
    "for text in test_sentences:\n",
    "    gloss = predict_gloss(text)\n",
    "    print(f\"\\nInput:  {text}\")\n",
    "    print(f\"Result: {gloss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
