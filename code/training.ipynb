{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2720c7d0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ead43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3 batches with 630 total pairs.\n",
      "Data split: {'train': 567, 'test': 63}\n",
      "Pruned Model loaded on: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 567/567 [00:00<00:00, 7927.74 examples/s]\n",
      "Map: 100%|██████████| 63/63 [00:00<00:00, 5980.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remapped 1210 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# 1. Files\n",
    "LV_PATTERN = \"../txt/latvian_sentences_*.txt\"\n",
    "GLOSS_PATTERN = \"../txt/lsl_glosses_*.txt\"\n",
    "PRUNED_MODEL_PATH = \"../mt5-pruned\"\n",
    "SAVE_DIR = \"../mt5-lsl-model\"\n",
    "\n",
    "# 2. Append all files to master list\n",
    "lv_lines = []\n",
    "gloss_lines = []\n",
    "total_files = 0\n",
    "\n",
    "for file_path in sorted(glob.glob(LV_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lv_lines.extend([line.strip() for line in f if line.strip()])\n",
    "    total_files += 1\n",
    "\n",
    "for file_path in sorted(glob.glob(GLOSS_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gloss_lines.extend([line.strip() for line in f if line.strip()])\n",
    "\n",
    "# 3. Check validity\n",
    "assert len(lv_lines) == len(gloss_lines), f\"❌ Mismatch! Sentence lines: {len(lv_lines)}, gloss lines: {len(gloss_lines)}\"\n",
    "print(f\"✅ Loaded {total_files} batches with {len(lv_lines)} total pairs.\")\n",
    "\n",
    "# 4. Calculate stats to save later\n",
    "all_lv_text = \" \".join(lv_lines)\n",
    "all_gloss_text = \" \".join(gloss_lines)\n",
    "unique_lv_words = len(set(all_lv_text.split()))\n",
    "unique_gloss_words = len(set(all_gloss_text.split()))\n",
    "\n",
    "# 5. Create a dataset, Train (90%) and Test (10%) split\n",
    "data = {\"lv\": lv_lines, \"gloss\": gloss_lines}\n",
    "raw_dataset = Dataset.from_dict(data)\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.1)\n",
    "print(\"Data split:\", split_dataset.num_rows)\n",
    "\n",
    "# 6. Load model\n",
    "tokenizer = T5Tokenizer.from_pretrained(PRUNED_MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(PRUNED_MODEL_PATH)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Pruned Model loaded on: {device}\")\n",
    "\n",
    "# 7. Vocabulary remap\n",
    "with open(os.path.join(PRUNED_MODEL_PATH, \"vocab_map.json\"), \"r\") as f:\n",
    "    new2old_map = json.load(f)\n",
    "    old2new_map = {v: int(k) for k, v in new2old_map.items()}\n",
    "\n",
    "original_unk_id = tokenizer.unk_token_id\n",
    "new_unk_id = old2new_map.get(original_unk_id, 0)\n",
    "\n",
    "def remap_tokens(token_ids):\n",
    "    return [old2new_map.get(tid, new_unk_id) for tid in token_ids]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"lv\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"gloss\"],\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    model_inputs[\"input_ids\"] = [remap_tokens(ids) for ids in model_inputs[\"input_ids\"]]\n",
    "    model_inputs[\"labels\"] = [remap_tokens(ids) for ids in labels[\"input_ids\"]]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(f\"Remapped {len(old2new_map)} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d768999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liene\\AppData\\Local\\Temp\\ipykernel_9404\\895998269.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# 8. Training parameters\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=50,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    predict_with_generate=True,\n",
    "    optim=\"adafactor\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f3f3f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='781' max='3550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 781/3550 06:28 < 23:01, 2.00 it/s, Epoch 11/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.151000</td>\n",
       "      <td>1.525597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.503800</td>\n",
       "      <td>0.654168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.096600</td>\n",
       "      <td>0.526155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.419393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.567300</td>\n",
       "      <td>0.410888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.364141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.320774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>0.304701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.381154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.216100</td>\n",
       "      <td>0.348261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.311869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "# 9. Train\n",
    "start_time = time.time()\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcb40578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ../mt5-lsl-model\n",
      "Cleaning up temporary checkpoints...\n",
      "✅ Metadata saved to: ../mt5-lsl-model\\model_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# 10. Save model\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"Saved model to {SAVE_DIR}\")\n",
    "\n",
    "# 11. Cleanup\n",
    "print(\"Cleaning up temporary checkpoints...\")\n",
    "for item in os.listdir(SAVE_DIR):\n",
    "    item_path = os.path.join(SAVE_DIR, item)\n",
    "    if os.path.isdir(item_path) and item.startswith(\"checkpoint-\"):\n",
    "        try:\n",
    "            shutil.rmtree(item_path)\n",
    "            print(f\"Deleted: {item}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not delete {item}: {e}\")\n",
    "\n",
    "# 12. Save metadata\n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "total_steps = train_result.global_step\n",
    "best_epoch_trained = None\n",
    "for log_entry in trainer.state.log_history:\n",
    "    if 'best_model_checkpoint' in log_entry:\n",
    "        if 'epoch' in log_entry:\n",
    "            best_epoch_trained = int(log_entry['epoch'])\n",
    "            break\n",
    "if best_epoch_trained is None and trainer.state.log_history:\n",
    "    last_log = trainer.state.log_history[-1]\n",
    "    if 'epoch' in last_log:\n",
    "        best_epoch_trained = int(last_log['epoch'])\n",
    "\n",
    "target_indices = [10, 60, 100, 160, 200, 260, 300, 360, 400, 460, 500, 560, 600, 660, 700]\n",
    "test_samples = []\n",
    "\n",
    "for idx in target_indices:\n",
    "    if idx < len(lv_lines):\n",
    "        test_samples.append(lv_lines[idx])\n",
    "\n",
    "sample_results = []\n",
    "model.eval()\n",
    "\n",
    "for text in test_samples:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    mapped_ids = [remap_tokens(ids) for ids in inputs.input_ids.tolist()]\n",
    "    input_tensor = torch.tensor(mapped_ids).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_tensor, max_new_tokens=128)\n",
    "    \n",
    "    out_ids = outputs[0].tolist()\n",
    "    orig_ids = [int(new2old_map.get(str(tid), tokenizer.unk_token_id)) for tid in out_ids]\n",
    "    decoded = tokenizer.decode(orig_ids, skip_special_tokens=True)\n",
    "    \n",
    "    sample_results.append({\"input\": text, \"output\": decoded})\n",
    "\n",
    "metadata = {\n",
    "    \"dataset_stats\": {\n",
    "        \"total_pairs\": len(lv_lines),\n",
    "        \"total_files\": total_files,\n",
    "        \"unique_sentence_words\": unique_lv_words,\n",
    "        \"unique_gloss_words\": unique_gloss_words,\n",
    "        \"tokens_remapped\": len(old2new_map),\n",
    "        \"train_split\": len(tokenized_datasets[\"train\"]),\n",
    "        \"test_split\": len(tokenized_datasets[\"test\"])\n",
    "    },\n",
    "    \"training_stats\": {\n",
    "        \"duration_seconds\": round(training_duration, 2),\n",
    "        \"epochs_planned\": args.num_train_epochs,\n",
    "        \"epochs_completed\": best_epoch_trained,\n",
    "        \"batch_size\": args.per_device_train_batch_size,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"final_train_loss\": train_result.training_loss,\n",
    "        \"final_eval_loss\": trainer.state.best_metric,\n",
    "        \"best_checkpoint_step\": trainer.state.best_model_checkpoint.split(\"-\")[-1] if trainer.state.best_model_checkpoint else total_steps\n",
    "    },\n",
    "    \"test_samples\": sample_results\n",
    "}\n",
    "\n",
    "# 8c. Save to JSON\n",
    "meta_path = os.path.join(SAVE_DIR, \"model_metadata.json\")\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Metadata saved to: {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62dc446",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be75ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "\n",
      "Input:  Čau!\n",
      "Result: sveiks\n",
      "\n",
      "Input:  Kāds ir jūsu vārds?\n",
      "Result: kāds ir tavs vārds\n",
      "\n",
      "Input:  Kāds ir tavs vārds?\n",
      "Result: kāds ir tavs vārds\n",
      "\n",
      "Input:  Priecājos ar Jums iepazīties!\n",
      "Result: prieks ar tavs iepazīties\n",
      "\n",
      "Input:  Man iet labi.\n",
      "Result: mans iet labs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP MAPS (Crucial!) ---\n",
    "# We need to load the map to convert between \"Big Tokenizer\" and \"Small Model\"\n",
    "vocab_map_path = os.path.join(PRUNED_MODEL_PATH, \"vocab_map.json\")\n",
    "\n",
    "with open(vocab_map_path, \"r\") as f:\n",
    "    new2old_map = json.load(f)\n",
    "    # We need both directions!\n",
    "    old2new_map = {int(v): int(k) for k, v in new2old_map.items()} # Big -> Small\n",
    "    new2old_map = {int(k): int(v) for k, v in new2old_map.items()} # Small -> Big\n",
    "\n",
    "original_unk_id = tokenizer.unk_token_id\n",
    "pruned_unk_id = old2new_map.get(original_unk_id, 0) # Fallback to 0 if not found\n",
    "\n",
    "\n",
    "# --- 2. CUSTOM TRANSLATION FUNCTION ---\n",
    "def predict_gloss(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    input_ids = inputs.input_ids[0].tolist()\n",
    "    \n",
    "    # Remap to pruned IDs\n",
    "    pruned_input_ids = [old2new_map.get(tid, pruned_unk_id) for tid in input_ids]\n",
    "    input_tensor = torch.tensor([pruned_input_ids]).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_tensor,\n",
    "            max_length=32,\n",
    "            num_beams=3,\n",
    "            early_stopping=True,\n",
    "            # no_repeat_ngram_size=2 # If we don't want many repeats!\n",
    "        )\n",
    "    \n",
    "    # Remap output IDs back to full tokenizer\n",
    "    output_ids = outputs[0].tolist()\n",
    "    original_output_ids = [new2old_map.get(tid, tokenizer.unk_token_id) for tid in output_ids]\n",
    "\n",
    "    # Decode\n",
    "    return tokenizer.decode(original_output_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# --- 3. RUN TESTS ---\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "\n",
    "test_sentences = [\n",
    "    lv_lines[0],\n",
    "    lv_lines[20],\n",
    "    lv_lines[30],\n",
    "    lv_lines[40],\n",
    "    lv_lines[50]\n",
    "]\n",
    "\n",
    "for text in test_sentences:\n",
    "    gloss = predict_gloss(text)\n",
    "    print(f\"\\nInput:  {text}\")\n",
    "    print(f\"Result: {gloss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
