{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2720c7d0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "501ead43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# 1. Config\n",
    "LV_PATTERN = \"../txt/latvian_sentences_*.txt\"\n",
    "GLOSS_PATTERN = \"../txt/lsl_glosses_*.txt\"\n",
    "PRUNED_MODEL_PATH = \"../models/mt5-pruned\"\n",
    "SAVE_DIR = \"../models/mt5-lsl-model\"\n",
    "\n",
    "VAL_RATIO = 0.08\n",
    "LOGGING_STEPS = 50\n",
    "TRAIN_EPOCHS = 30\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-3 # Try 3e-4 or 5e-4\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_NEW_TOKENS = 128\n",
    "RARE_THRESHOLD = 2\n",
    "OPTIMIZER = \"adafactor\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72446ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 9 batches with 1914 total pairs.\n",
      "ğŸ“¦ Training run #4 â†’ ../models/mt5-lsl-model\\4\n",
      "\n",
      "âœ… Dataset split:\n",
      "  Train: 1782 (93.10%)\n",
      "  Validation: 132 (6.90%)\n",
      "\n",
      "Glosses appearing 1 time: 157\n",
      "Glosses appearing 2 times: 179\n",
      "Examples of 1 time glosses:\n",
      "   atÄ¼auja\n",
      "   steigties\n",
      "   pieklÄjÄ«ba\n",
      "   miegs\n",
      "   kleita\n",
      "\n",
      "Validation samples containing rare glosses (â‰¤2): 11\n",
      "  'pÄrraut'\n",
      "     LV:    ZÄÄ¼u raÅ¾otÄji informÄ“ ZÄÄ¼u valsts aÄ£entÅ«ru par zÄÄ¼u piegÄÅ¾u pÄrrÄvumiem â€” kuras zÄles nevarÄ“s piegÄdÄt un kad piegÄdi atjaunos.\n",
      "     GLOSS: zÄles raÅ¾ot informÄcija zÄles valsts iestÄde par zÄles piegÄde pÄrraut kurÅ¡ zÄles nevarÄ“t piegÄde un kad piegÄde atjaunot\n",
      "  'skriet'\n",
      "     LV:    VarbÅ«t skriet, varbÅ«t iet, bet varbÅ«t braukt.\n",
      "     GLOSS: varbÅ«t skriet varbÅ«t iet bet varbÅ«t braukt\n",
      "  'luksofors'\n",
      "     LV:    SarkanÄ, dzeltenÄ un zaÄ¼Ä luksofora krÄsas.\n",
      "     GLOSS: sarkans dzeltens zaÄ¼Å¡ luksofors krÄsa\n"
     ]
    }
   ],
   "source": [
    "# 2. Append all files to master list\n",
    "lv_lines = []\n",
    "gloss_lines = []\n",
    "total_files = 0\n",
    "\n",
    "for file_path in sorted(glob.glob(LV_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lv_lines.extend([line.strip() for line in f if line.strip()])\n",
    "    total_files += 1\n",
    "\n",
    "for file_path in sorted(glob.glob(GLOSS_PATTERN)):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gloss_lines.extend([line.strip() for line in f if line.strip()])\n",
    "\n",
    "assert len(lv_lines) == len(gloss_lines), f\"âŒ Mismatch! Sentence lines: {len(lv_lines)}, gloss lines: {len(gloss_lines)}\"\n",
    "print(f\"âœ… Loaded {total_files} batches with {len(lv_lines)} total pairs.\")\n",
    "\n",
    "\n",
    "# 3. Index\n",
    "def get_next_run_dir(base_dir: str):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    existing = [\n",
    "        int(d) for d in os.listdir(base_dir)\n",
    "        if d.isdigit() and os.path.isdir(os.path.join(base_dir, d))\n",
    "    ]\n",
    "\n",
    "    next_id = max(existing, default=0) + 1\n",
    "    run_dir = os.path.join(base_dir, f\"{next_id}\")\n",
    "    os.makedirs(run_dir)\n",
    "\n",
    "    return next_id, run_dir\n",
    "\n",
    "RUN_ID, RUN_DIR = get_next_run_dir(SAVE_DIR)\n",
    "print(f\"ğŸ“¦ Training run #{RUN_ID} â†’ {RUN_DIR}\\n\")\n",
    "\n",
    "\n",
    "# 4. Create a dataset, Train (90%) and Test (10%) split\n",
    "all_lv_text = \" \".join(lv_lines)\n",
    "all_gloss_text = \" \".join(gloss_lines)\n",
    "unique_lv_words = len(set(all_lv_text.split()))\n",
    "unique_gloss_words = len(set(all_gloss_text.split()))\n",
    "\n",
    "data = {\"lv\": lv_lines, \"gloss\": gloss_lines}\n",
    "\n",
    "gloss_to_indices = defaultdict(list)\n",
    "\n",
    "for idx, gloss_line in enumerate(gloss_lines):\n",
    "    for gloss in gloss_line.split():\n",
    "        gloss_to_indices[gloss].append(idx)\n",
    "\n",
    "forced_train_indices = set()\n",
    "\n",
    "for gloss, indices in gloss_to_indices.items():\n",
    "    if len(indices) <= RARE_THRESHOLD:\n",
    "        forced_train_indices.add(indices[0])\n",
    "\n",
    "all_indices = set(range(len(lv_lines)))\n",
    "remaining_indices = list(all_indices - forced_train_indices)\n",
    "\n",
    "val_size = int(round(VAL_RATIO * len(remaining_indices)))\n",
    "\n",
    "val_indices = set(random.sample(\n",
    "    remaining_indices,\n",
    "    min(val_size, len(remaining_indices))\n",
    "))\n",
    "\n",
    "train_indices = list(all_indices - val_indices)\n",
    "val_indices = list(val_indices)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"lv\": [lv_lines[i] for i in train_indices],\n",
    "    \"gloss\": [gloss_lines[i] for i in train_indices],\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"lv\": [lv_lines[i] for i in val_indices],\n",
    "    \"gloss\": [gloss_lines[i] for i in val_indices],\n",
    "})\n",
    "\n",
    "total = len(train_indices) + len(val_indices)\n",
    "\n",
    "train_pct = 100 * len(train_indices) / total\n",
    "val_pct = 100 * len(val_indices) / total\n",
    "\n",
    "print(f\"âœ… Dataset split:\")\n",
    "print(f\"  Train: {len(train_indices)} ({train_pct:.2f}%)\")\n",
    "print(f\"  Validation: {len(val_indices)} ({val_pct:.2f}%)\\n\")\n",
    "\n",
    "\n",
    "# 5. Check dataset validity\n",
    "train_gloss_vocab = set()\n",
    "for g in train_dataset[\"gloss\"]:\n",
    "    train_gloss_vocab.update(g.split())\n",
    "\n",
    "missing = set(gloss_to_indices.keys()) - train_gloss_vocab\n",
    "assert not missing, f\"Missing glosses in training: {missing}\"\n",
    "\n",
    "single_occurrence_glosses = {\n",
    "    gloss: indices\n",
    "    for gloss, indices in gloss_to_indices.items()\n",
    "    if len(indices) == 1\n",
    "}\n",
    "\n",
    "double_occurrence_glosses = {\n",
    "    gloss: indices\n",
    "    for gloss, indices in gloss_to_indices.items()\n",
    "    if len(indices) == 2\n",
    "}\n",
    "\n",
    "print(f\"Glosses appearing 1 time: {len(single_occurrence_glosses)}\")\n",
    "print(f\"Glosses appearing 2 times: {len(double_occurrence_glosses)}\")\n",
    "\n",
    "if single_occurrence_glosses:\n",
    "    print(\"Examples of 1 time glosses:\")\n",
    "    for g in list(single_occurrence_glosses.keys())[:5]:\n",
    "        print(\"  \", g)\n",
    "\n",
    "rare_val_examples = []\n",
    "single_val_examples = []\n",
    "\n",
    "for idx in val_indices:\n",
    "    glosses = gloss_lines[idx].split()\n",
    "    for g in glosses:\n",
    "        freq = len(gloss_to_indices[g])\n",
    "\n",
    "        if freq <= RARE_THRESHOLD:\n",
    "            rare_val_examples.append((idx, g, freq))\n",
    "\n",
    "        if freq == 1:\n",
    "            single_val_examples.append((idx, g))\n",
    "\n",
    "print(f\"\\nValidation samples containing rare glosses (â‰¤{RARE_THRESHOLD}): {len(rare_val_examples)}\")\n",
    "\n",
    "if rare_val_examples:\n",
    "    for idx, gloss, freq in rare_val_examples[:3]:\n",
    "        print(f\"  '{gloss}'\")\n",
    "        print(f\"     LV:    {lv_lines[idx]}\")\n",
    "        print(f\"     GLOSS: {gloss_lines[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c365c99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model loaded on: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dad8c6eeb444c0b4291716ba5574b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefe0a73cc394b69821f9656d6316e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remapped 2974 tokens.\n"
     ]
    }
   ],
   "source": [
    "# 6. Load model\n",
    "tokenizer = T5Tokenizer.from_pretrained(PRUNED_MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(PRUNED_MODEL_PATH)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Pruned Model loaded on: {device}\")\n",
    "\n",
    "# 7. Vocabulary remap\n",
    "with open(os.path.join(PRUNED_MODEL_PATH, \"vocab_map.json\"), \"r\") as f:\n",
    "    new2old_map = json.load(f)\n",
    "    old2new_map = {v: int(k) for k, v in new2old_map.items()}\n",
    "\n",
    "original_unk_id = tokenizer.unk_token_id\n",
    "new_unk_id = old2new_map.get(original_unk_id, 0)\n",
    "\n",
    "def remap_tokens(token_ids):\n",
    "    return [old2new_map.get(tid, new_unk_id) for tid in token_ids]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"lv\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_NEW_TOKENS, truncation=True)\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"gloss\"],\n",
    "        max_length=MAX_NEW_TOKENS,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    model_inputs[\"input_ids\"] = [remap_tokens(ids) for ids in model_inputs[\"input_ids\"]]\n",
    "    model_inputs[\"labels\"] = [remap_tokens(ids) for ids in labels[\"input_ids\"]]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"lv\", \"gloss\"]\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"lv\", \"gloss\"]\n",
    ")\n",
    "\n",
    "print(f\"Remapped {len(old2new_map)} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d768999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liene\\AppData\\Local\\Temp\\ipykernel_11932\\3517825860.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# 8. Training parameters\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=RUN_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_total_limit=1,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    predict_with_generate=True,\n",
    "    optim=OPTIMIZER,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f3f3f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3345' max='6690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3345/6690 34:16 < 34:17, 1.63 it/s, Epoch 15/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.241900</td>\n",
       "      <td>2.727356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.160500</td>\n",
       "      <td>0.955298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.264600</td>\n",
       "      <td>0.667086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.584804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.639300</td>\n",
       "      <td>0.529693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.431287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.401445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.370321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.322822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.209900</td>\n",
       "      <td>0.347867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.190600</td>\n",
       "      <td>0.340275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.313982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.317478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.342827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>0.337222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "# 9. Train (apprx. 28 mins)\n",
    "start_time = time.time()\n",
    "train_result = trainer.train()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cee8df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ../models/mt5-lsl-model\\4\n",
      "Cleaning up temporary checkpoints...\n",
      "Deleted: checkpoint-2676\n"
     ]
    }
   ],
   "source": [
    "# 10. Save model\n",
    "model.save_pretrained(RUN_DIR)\n",
    "tokenizer.save_pretrained(RUN_DIR)\n",
    "\n",
    "print(f\"Saved model to {RUN_DIR}\")\n",
    "\n",
    "\n",
    "# 11. Cleanup\n",
    "print(\"Cleaning up temporary checkpoints...\")\n",
    "for item in os.listdir(RUN_DIR):\n",
    "    item_path = os.path.join(RUN_DIR, item)\n",
    "    if os.path.isdir(item_path) and item.startswith(\"checkpoint-\"):\n",
    "        try:\n",
    "            shutil.rmtree(item_path)\n",
    "            print(f\"Deleted: {item}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not delete {item}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcb40578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metadata saved to: ../models/mt5-lsl-model\\4\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# 12. Generate metadata\n",
    "\n",
    "# 12a. Time\n",
    "current_time = time.ctime()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "# 12b. Metrics of best saved model\n",
    "best_epoch = None\n",
    "best_train_loss = None\n",
    "best_eval_loss = trainer.state.best_metric\n",
    "\n",
    "best_checkpoint = trainer.state.best_model_checkpoint\n",
    "best_step = int(best_checkpoint.split(\"-\")[-1]) if best_checkpoint else None\n",
    "\n",
    "for entry in trainer.state.log_history:\n",
    "    if \"eval_loss\" in entry and abs(entry[\"eval_loss\"] - best_eval_loss) < 1e-6:\n",
    "        best_epoch = entry.get(\"epoch\")\n",
    "\n",
    "    if best_step is not None and \"loss\" in entry and entry[\"step\"] <= best_step:\n",
    "        best_train_loss = entry[\"loss\"]\n",
    "\n",
    "# 12c. How many epochs were trained\n",
    "total_epoch = None\n",
    "total_steps = train_result.global_step\n",
    "\n",
    "for entry in trainer.state.log_history:\n",
    "    if \"epoch\" in entry:\n",
    "        total_epoch = int(entry[\"epoch\"])\n",
    "\n",
    "if total_epoch is None and trainer.state.log_history:\n",
    "    last_log = trainer.state.log_history[-1]\n",
    "    if 'epoch' in last_log:\n",
    "        total_epoch = int(last_log['epoch'])\n",
    "\n",
    "# 12d. Sentence examples\n",
    "NUM_EXAMPLES = 6\n",
    "\n",
    "train_example_indices = random.sample(train_indices, min(NUM_EXAMPLES, len(train_indices)))\n",
    "val_example_indices = random.sample(val_indices, min(NUM_EXAMPLES, len(val_indices)))\n",
    "\n",
    "def generate_examples(indices):\n",
    "    results = []\n",
    "    model.eval()\n",
    "\n",
    "    for idx in indices:\n",
    "        text = lv_lines[idx]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=MAX_NEW_TOKENS,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        mapped_ids = [remap_tokens(ids) for ids in inputs.input_ids.tolist()]\n",
    "        input_tensor = torch.tensor(mapped_ids).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_tensor, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "        out_ids = outputs[0].tolist()\n",
    "        orig_ids = [\n",
    "            int(new2old_map.get(str(tid), tokenizer.unk_token_id))\n",
    "            for tid in out_ids\n",
    "        ]\n",
    "\n",
    "        decoded = tokenizer.decode(orig_ids, skip_special_tokens=True)\n",
    "\n",
    "        results.append({\n",
    "            \"input\": text,\n",
    "            \"output\": decoded,\n",
    "            \"target\": gloss_lines[idx]\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "train_sample_results = []\n",
    "validation_sample_results = []\n",
    "train_sample_results += generate_examples(train_example_indices)\n",
    "validation_sample_results += generate_examples(val_example_indices)\n",
    "\n",
    "# 12e. Metadata structure\n",
    "metadata = {\n",
    "    \"general_info\": {\n",
    "        \"id\": RUN_ID,\n",
    "        \"seed\": SEED,\n",
    "        \"base_model\" : \"google/mt5-small\",\n",
    "        \"time_completed\": current_time\n",
    "    },\n",
    "    \"dataset_stats\": {\n",
    "        \"total_pairs\": len(lv_lines),\n",
    "        \"total_files\": total_files,\n",
    "        \"unique_sentence_words\": unique_lv_words,\n",
    "        \"unique_gloss_words\": unique_gloss_words,\n",
    "        \"tokens_remapped\": len(old2new_map),\n",
    "    },\n",
    "    \"dataset_split\": {\n",
    "        \"rare_gloss_threshold\": RARE_THRESHOLD,\n",
    "        \"train_split\": len(tokenized_train),\n",
    "        \"validation_split\": len(tokenized_val),\n",
    "        \"train_split_%\": f\"{train_pct:.2f}%\",\n",
    "        \"validation_split_%\": f\"{val_pct:.2f}%\"\n",
    "    },\n",
    "    \"generation\": {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"decoder_start_token_id\": model.config.decoder_start_token_id\n",
    "    },\n",
    "    \"training_stats\": {\n",
    "        \"duration_seconds\": round(training_duration, 2),\n",
    "        \"epochs_completed\": total_epoch,\n",
    "        \"batch_size\": args.per_device_train_batch_size,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "        \"early_stopping_patience\": EARLY_STOPPING_PATIENCE,\n",
    "        \"epochs_planned\": args.num_train_epochs,\n",
    "        \"optimizer\": OPTIMIZER,\n",
    "        \"mean_train_loss_over_all_steps\": train_result.training_loss,\n",
    "    },\n",
    "    \"final_model\": {\n",
    "        \"epoch\": best_epoch,\n",
    "        \"train_loss\": best_train_loss,\n",
    "        \"eval_loss\": best_eval_loss,\n",
    "        \"checkpoint_step\": trainer.state.best_model_checkpoint.split(\"-\")[-1] if trainer.state.best_model_checkpoint else total_steps\n",
    "    },\n",
    "    \"test_samples\": {\n",
    "        \"train\": train_sample_results,\n",
    "        \"validation\": validation_sample_results\n",
    "    }\n",
    "}\n",
    "\n",
    "# 12f. Save to JSON\n",
    "meta_path = os.path.join(RUN_DIR, \"metadata.json\")\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Metadata saved to: {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62dc446",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7be75ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "\n",
      "Input:  Sveiks, [NAME].\n",
      "Result: sveiks [NAME]\n",
      "\n",
      "Input:  KÄdÄ vÄrdÄ tevi saukt?\n",
      "Result: kÄds vÄrds tu saukt\n",
      "\n",
      "Input:  [NAME] ir Å¡Ä«s personas vÄrds.\n",
      "Result: [NAME] ir tas cilvÄ“ks vÄrds\n",
      "\n",
      "Input:  Es negribu tevi redzÄ“t...\n",
      "Result: es negribÄ“t tu redzÄ“t\n",
      "\n",
      "Input:  NovÄ“lam Jums veiksmi!\n",
      "Result: novÄ“lÄ“t tavs veiksme\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP MAPS (Crucial!) ---\n",
    "# We need to load the map to convert between \"Big Tokenizer\" and \"Small Model\"\n",
    "vocab_map_path = os.path.join(PRUNED_MODEL_PATH, \"vocab_map.json\")\n",
    "\n",
    "with open(vocab_map_path, \"r\") as f:\n",
    "    new2old_map = json.load(f)\n",
    "    # We need both directions!\n",
    "    old2new_map = {int(v): int(k) for k, v in new2old_map.items()} # Big -> Small\n",
    "    new2old_map = {int(k): int(v) for k, v in new2old_map.items()} # Small -> Big\n",
    "\n",
    "original_unk_id = tokenizer.unk_token_id\n",
    "pruned_unk_id = old2new_map.get(original_unk_id, 0) # Fallback to 0 if not found\n",
    "\n",
    "\n",
    "# --- 2. CUSTOM TRANSLATION FUNCTION ---\n",
    "def predict_gloss(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_NEW_TOKENS)\n",
    "    input_ids = inputs.input_ids[0].tolist()\n",
    "    \n",
    "    # Remap to pruned IDs\n",
    "    pruned_input_ids = [old2new_map.get(tid, pruned_unk_id) for tid in input_ids]\n",
    "    input_tensor = torch.tensor([pruned_input_ids]).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_tensor,\n",
    "            max_length=32,\n",
    "            num_beams=3,\n",
    "            early_stopping=True,\n",
    "            # no_repeat_ngram_size=2 # If we don't want many repeats!\n",
    "        )\n",
    "    \n",
    "    # Remap output IDs back to full tokenizer\n",
    "    output_ids = outputs[0].tolist()\n",
    "    original_output_ids = [new2old_map.get(tid, tokenizer.unk_token_id) for tid in output_ids]\n",
    "\n",
    "    # Decode\n",
    "    return tokenizer.decode(original_output_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# --- 3. RUN TESTS ---\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "\n",
    "test_sentences = [\n",
    "    lv_lines[5],\n",
    "    lv_lines[25],\n",
    "    lv_lines[35],\n",
    "    lv_lines[45],\n",
    "    lv_lines[55]\n",
    "]\n",
    "\n",
    "for text in test_sentences:\n",
    "    gloss = predict_gloss(text)\n",
    "    print(f\"\\nInput:  {text}\")\n",
    "    print(f\"Result: {gloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff81c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.164\n",
      "LV:    PulksteÅ†a laiks ir divpadsmit.\n",
      "GOLD:  pulkstenis laiks ir 12\n",
      "\n",
      "Loss: 1.846\n",
      "LV:    MainÄ«sim darbu.\n",
      "GOLD:  mainÄ«t darbs\n",
      "\n",
      "Loss: 1.784\n",
      "LV:    Dabas katastrofas arÄ« citur Eiropas valstÄ«s.\n",
      "GOLD:  daba avÄrija arÄ« cits eiropa valsts\n",
      "\n",
      "Loss: 1.735\n",
      "LV:    Nekas Å¡ausmÄ«gs, paÅ†emsim taksi.\n",
      "GOLD:  nekas Å¡ausmas paÅ†emt taksometrs\n",
      "\n",
      "Loss: 1.657\n",
      "LV:    TrÄ«sdesmit pirmais augusts tiek uzskatÄ«ts par vasaras beigÄm.\n",
      "GOLD:  30 pirmais augusts ir uzskatÄms par vasara beigas\n",
      "\n",
      "Loss: 1.582\n",
      "LV:    Prieks tevi satikt.\n",
      "GOLD:  prieks tu tikties\n",
      "\n",
      "Loss: 1.138\n",
      "LV:    ViÅ†as nodarbojas ar zemkopÄ«bu.\n",
      "GOLD:  viÅ†i nodarboties ar zeme kopt\n",
      "\n",
      "Loss: 1.118\n",
      "LV:    SarkanÄ, dzeltenÄ un zaÄ¼Ä luksofora krÄsas.\n",
      "GOLD:  sarkans dzeltens zaÄ¼Å¡ luksofors krÄsa\n",
      "\n",
      "Loss: 0.986\n",
      "LV:    RÄ«gas DzemdÄ«bu namÄ Å¡ajÄ nedÄ“Ä¼Ä divas Ä£imenes sagaidÄ«juÅ¡as dvÄ«nÄ«Å¡us.\n",
      "GOLD:  rÄ«ga dzimt mÄja Å¡odien nedÄ“Ä¼a 2 Ä£imene gaidÄ«t dvÄ«nis\n",
      "\n",
      "Loss: 0.968\n",
      "LV:    Å orÄ«t temperatÅ«ra man bija [NUM].\n",
      "GOLD:  Å¡odien rÄ«ts temperatÅ«ra_veselÄ«ba bija [NUM]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()\n",
    "sample_losses = []\n",
    "\n",
    "for i in range(len(tokenized_val)):\n",
    "    batch = tokenized_val[i]\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    labels = torch.tensor(batch[\"labels\"]).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "    loss = outputs.loss.item()\n",
    "    sample_losses.append((i, loss))\n",
    "\n",
    "hardest = sorted(sample_losses, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "for idx, loss in hardest:\n",
    "    print(f\"Loss: {loss:.3f}\")\n",
    "    print(\"LV:   \", val_dataset[idx][\"lv\"])\n",
    "    print(\"GOLD: \", val_dataset[idx][\"gloss\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5943d9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently missed glosses:\n",
      "[('ir', 26), ('es', 11), ('un', 10), ('ar', 9), ('tas', 9), ('tu', 8), ('labs', 8), ('mans', 8), ('vai', 8), ('[NUM]', 7)]\n",
      "\n",
      "Most frequently hallucinated glosses:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "missing_gloss = Counter()\n",
    "extra_gloss = Counter()\n",
    "\n",
    "for i in range(len(val_dataset)):\n",
    "    src = val_dataset[i][\"lv\"]\n",
    "    gold = val_dataset[i][\"gloss\"].split()\n",
    "\n",
    "    inputs = tokenizer(src, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    mapped_ids = [remap_tokens(ids) for ids in inputs.input_ids.tolist()]\n",
    "    input_tensor = torch.tensor(mapped_ids).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_tensor, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    out_ids = out[0].tolist()\n",
    "    orig_ids = [int(new2old_map.get(str(tid), tokenizer.unk_token_id)) for tid in out_ids]\n",
    "    pred = tokenizer.decode(orig_ids, skip_special_tokens=True).split()\n",
    "\n",
    "    gold_c = Counter(gold)\n",
    "    pred_c = Counter(pred)\n",
    "\n",
    "    # Missing tokens (under-produced)\n",
    "    for g, cnt in (gold_c - pred_c).items():\n",
    "        missing_gloss[g] += cnt\n",
    "\n",
    "    # Extra tokens (over-produced / hallucinated)\n",
    "    for g, cnt in (pred_c - gold_c).items():\n",
    "        extra_gloss[g] += cnt\n",
    "\n",
    "print(\"Most frequently missed glosses:\")\n",
    "print(missing_gloss.most_common(10))\n",
    "\n",
    "print(\"\\nMost frequently hallucinated glosses:\")\n",
    "print(extra_gloss.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
