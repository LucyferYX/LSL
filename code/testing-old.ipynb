{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e45caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liene\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PRUNED_MODEL_PATH = \"../mt5-pruned\"\n",
    "TRAINED_MODEL_PATH = \"../mt5-lsl-model\"\n",
    "\n",
    "# --- 1. HELPER FUNCTIONS (Logic) ---\n",
    "\n",
    "def expand_number(number_str):\n",
    "    \"\"\"Converts '24' -> '20 4', '1990' -> '1000 900 90'\"\"\"\n",
    "    # Simple logic: remove non-digits just in case\n",
    "    s = re.sub(r\"\\D\", \"\", str(number_str))\n",
    "    if not s: return number_str\n",
    "    \n",
    "    length = len(s)\n",
    "    parts = []\n",
    "    for i, digit in enumerate(s):\n",
    "        if digit != '0':\n",
    "            # Calculate place value (e.g., 2 * 10^1 = 20)\n",
    "            place_value = int(digit) * (10**(length - i - 1))\n",
    "            parts.append(str(place_value))\n",
    "    \n",
    "    # If the number was just \"0\" or \"00\", handle it\n",
    "    if not parts and \"0\" in s: return \"0\"\n",
    "    \n",
    "    return \" \".join(parts)\n",
    "\n",
    "def fingerspell(name_str):\n",
    "    \"\"\"Converts 'Jānis' -> 'j ā n i s'\"\"\"\n",
    "    return \" \".join(list(name_str.lower()))\n",
    "\n",
    "# --- 2. THE TRANSLATOR CLASS ---\n",
    "\n",
    "class LSLTranslator:\n",
    "    def __init__(self, pruned_path, trained_path):\n",
    "        print(\"Loading system...\")\n",
    "        \n",
    "        # A. Load the Map (Rosetta Stone)\n",
    "        map_path = os.path.join(pruned_path, \"vocab_map.json\")\n",
    "        with open(map_path, \"r\") as f:\n",
    "            new2old = json.load(f)\n",
    "            # Create both directions\n",
    "            self.new2old_map = {int(k): int(v) for k, v in new2old.items()} # Small -> Big\n",
    "            self.old2new_map = {v: k for k, v in self.new2old_map.items()} # Big -> Small\n",
    "\n",
    "        # B. Load Tokenizer (Original Big Tokenizer)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(trained_path)\n",
    "        \n",
    "        # C. Load Model (Tiny Pruned Architecture)\n",
    "        config = AutoConfig.from_pretrained(trained_path)\n",
    "        # Ensure config knows the small vocab size\n",
    "        config.vocab_size = len(self.new2old_map) \n",
    "        \n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(trained_path, config=config)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # D. Define UNK IDs\n",
    "        self.original_unk_id = self.tokenizer.unk_token_id\n",
    "        self.pruned_unk_id = self.old2new_map.get(self.original_unk_id, 0)\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Runs the raw text through the model using the remapping logic.\n",
    "        Input: \"translate Latvian to Gloss: [NAME] nopirka [NUM] kreklus.\"\n",
    "        Output: \"[NAME] pirkt [NUM] krekls\"\n",
    "        \"\"\"\n",
    "        # 1. Tokenize (Big IDs)\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "        input_ids = inputs.input_ids[0].tolist()\n",
    "\n",
    "        # 2. Remap (Big -> Small)\n",
    "        pruned_ids = [self.old2new_map.get(tid, self.pruned_unk_id) for tid in input_ids]\n",
    "        input_tensor = torch.tensor([pruned_ids]).to(self.device)\n",
    "\n",
    "        # 3. Generate (Model works in Small IDs)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(input_tensor, max_new_tokens=128)\n",
    "\n",
    "        # 4. Remap (Small -> Big)\n",
    "        output_ids = outputs[0].tolist()\n",
    "        original_ids = [self.new2old_map.get(tid, self.original_unk_id) for tid in output_ids]\n",
    "\n",
    "        # 5. Decode\n",
    "        return self.tokenizer.decode(original_ids, skip_special_tokens=True)\n",
    "\n",
    "# --- 3. THE \"WRAPPER\" LOGIC ---\n",
    "\n",
    "def run_full_translation(translator, user_input):\n",
    "    print(f\"\\nUser Input: '{user_input}'\")\n",
    "    \n",
    "    # --- STEP A: PRE-PROCESSING (Extract & Replace) ---\n",
    "    processed_input = user_input\n",
    "    variables = {}\n",
    "\n",
    "    # 1. Handle Numbers (Regex for digits)\n",
    "    # We find all numbers, e.g., \"24\"\n",
    "    numbers = re.findall(r'\\b\\d+\\b', processed_input)\n",
    "    if numbers:\n",
    "        # For simplicity in this demo, we handle the first number found.\n",
    "        # In a full app, you'd loop through them with unique IDs like [NUM1], [NUM2]\n",
    "        num_val = numbers[0]\n",
    "        processed_input = processed_input.replace(num_val, \"[NUM]\", 1)\n",
    "        variables[\"[NUM]\"] = expand_number(num_val) # Store \"20 4\"\n",
    "\n",
    "    # 2. Handle Names (Heuristic for Demo)\n",
    "    # NOTE: For production, use Spacy or a list of known names.\n",
    "    # Here, we assume \"Jānis\" is a name because you mentioned it.\n",
    "    known_names = [\"Jānis\", \"Anna\", \"Pēteris\", \"Oto\"] \n",
    "    found_name = None\n",
    "    \n",
    "    for name in known_names:\n",
    "        if name in processed_input:\n",
    "            found_name = name\n",
    "            break\n",
    "            \n",
    "    if found_name:\n",
    "        processed_input = processed_input.replace(found_name, \"[NAME]\", 1)\n",
    "        variables[\"[NAME]\"] = fingerspell(found_name) # Store \"j ā n i s\"\n",
    "\n",
    "    print(f\"Sent to Model: '{processed_input}'\")\n",
    "\n",
    "    # --- STEP B: MODEL INFERENCE ---\n",
    "    # (Optional: Add the prefix if your model relies on it, otherwise leave raw)\n",
    "    # model_input_text = \"translate Latvian to Gloss: \" + processed_input \n",
    "    model_input_text = processed_input # Assuming we removed prefix\n",
    "    \n",
    "    gloss_output = translator.predict(model_input_text)\n",
    "    print(f\"Model Raw Output: '{gloss_output}'\")\n",
    "\n",
    "    # --- STEP C: POST-PROCESSING (Inject Back) ---\n",
    "    final_output = gloss_output\n",
    "    \n",
    "    # Swap [NAME] -> \"j ā n i s\"\n",
    "    if \"[NAME]\" in final_output and \"[NAME]\" in variables:\n",
    "        final_output = final_output.replace(\"[NAME]\", variables[\"[NAME]\"])\n",
    "        \n",
    "    # Swap [NUM] -> \"20 4\"\n",
    "    if \"[NUM]\" in final_output and \"[NUM]\" in variables:\n",
    "        final_output = final_output.replace(\"[NUM]\", variables[\"[NUM]\"])\n",
    "\n",
    "    print(f\"Final Result: '{final_output}'\")\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b3457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '../mt5-lsl-model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Input: 'Jānis nopirka 24 kreklus.'\n",
      "Sent to Model: '[NAME] nopirka [NUM] kreklus.'\n",
      "Model Raw Output: '[NAME] krelēt [NAME] krelēt'\n",
      "Final Result: 'j ā n i s krelēt j ā n i s krelēt'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    system = LSLTranslator(PRUNED_MODEL_PATH, TRAINED_MODEL_PATH)\n",
    "    \n",
    "    # Test Case\n",
    "    text = \"Jānis nopirka 24 kreklus.\"\n",
    "    run_full_translation(system, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
